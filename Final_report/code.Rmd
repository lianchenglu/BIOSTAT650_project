---
title: "peoject"
author: "Liancheng, He Zhang, Zhengrui Huang, Zibo Yu"
date: "2023-12-07"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

complete vs incomplete ########################################

Set working directory

Write your own code to set your current working directory to a folder with all files extracted from “Data.zip”

Example: setwd("~/Documents/Biostat 650/project")

Read data ##################

```{r warning=FALSE}
library(NHANES)
library(dplyr)
df0 <- NHANES
df <- NHANES[NHANES$Age >= 18 & NHANES$Age < 60,]
df <- df[!duplicated(df),]
```

(1) Data cleaning ########################################

Create a subset with only varaibles of interest: Age, Comorbidity1, CurrentSmoker, Depression, Fatalism, HiChol, Htn, NIHScore, Optimism, Pessimism, R_E, Sex, Spirituality

```{r}
data2 <- df %>% select(
  SleepHrsNight,
  BMI,
  Age,
  Gender,
  Race1,
  TotChol,
  BPDiaAve,
  BPSysAve,
  AlcoholYear,
  Poverty,
  DaysMentHlthBad,
  UrineFlow1,
  PhysActive,
  DaysPhysHlthBad,
  Smoke100,
  HealthGen
)
```

"CurrentSmoker", "HiChol", "Htn", "R_E", "Sex" are binary variables, so set them as factors

data type

```{r}
data2$Gender <- ifelse(data2$Gender == "male", 0, 1)
data2$Smoke100 <- ifelse(data2$Smoke100 == "No", 0, 1)
data2$PhysActive <- ifelse(data2$PhysActive == "No", 0, 1)
data2 <- data2 %>%
  mutate(
    Race1 = case_when(
      Race1 == 'Black' ~ 1,
      Race1 == 'Hispanic' ~ 2,
      Race1 == 'Mexican' ~ 3,
      Race1 == 'White' ~ 4,
      Race1 == 'Other' ~ 5,
      TRUE ~ NA_integer_  # Default value if none of the conditions are met
    )
  )
data2 <- data2 %>%
  mutate(
    HealthGen = case_when(
      HealthGen == 'Poor' ~ 1,
      HealthGen == 'Fair' ~ 2,
      HealthGen == 'Good' ~ 3,
      HealthGen == 'Vgood' ~ 4,
      HealthGen == 'Excellent' ~ 5,
      TRUE ~ NA_integer_  # Default value if none of the conditions are met
    )
  )
data2$PhysActive = as.factor(data2$PhysActive)
data2$Smoke100 = as.factor(data2$Smoke100)
data2$HealthGen = as.factor(data2$HealthGen)
data2$Race1 = as.factor(data2$Race1)
data2$Gender = as.factor(data2$Gender)
```

Create an indicator variable “Incomplete” (0 if all variables are complete; 1 if at least one variable is missing)

```{r}
miss_num = apply(data2, 1, function(x) sum(is.na(x)))
data2$Incomplete = ifelse(miss_num>0, 1, 0)
data2$Incomplete = as.factor(data2$Incomplete)
```

(2) Compare complete to non-complete cases with graphs and t/χ2 tests ################

Create two categorical variables

a) Create “Age_4Cat” with 3 levels (0/1/2/3):20, 40, 60

```{r}
data2$Age_4Cat = cut(data2$Age, c(20, 40, 60, max(data2$Age, na.rm = T)), right=T, labels=c(0:2))
```

b) Create “BMI_cat” with 3 levels (0/1/2/3): 18, 25, 30

```{r}
data2$BMI_cat = cut(data2$BMI, c( 20, 40, 60, max(data2$BMI, na.rm = T)), right=F, labels=c(0:2))
```

(c) Create a new dataset with only complete data; Create another new dataset with only incomplete data ################

```{r}
data_complete = subset(data2, data2$Incomplete == 0)
data_incomplete = subset(data2, data2$Incomplete == 1)
```

(d) null hypothesis: H0:p1=p2  Ha: H1:$p1 \neq p2$; Decision: Do not reject H0

```{r}
n1f=2710
p1=sum(data_complete$BMI)/n1f
n2f=1509
p2=sum(data_incomplete$BMI)/n2f
f_diff=p1-p2
se_f=sqrt(p1*(1-p1)/n1f+p2*(1-p2)/n2f)
z <- f_diff/se_f
p_value_f <- 2 * (1 - pnorm(abs(z)))
low_bound_diff_f <- f_diff - 1.96 * se_f
high_bound_diff_f <- f_diff + 1.96 * se_f
```

(e) Use t-tests (χ2 tests) to compare differences in the mean (or proportion) of a given variable comparing complete cases to incomplete cases

Continuous variables: get mean(sd) and p-values from two-sample t-tests

Prepare data

```{r}
Cont_vars = c( "BMI",
               "SleepHrsNight",
               "Age",
               "TotChol",
               "BPDiaAve",
               "BPSysAve",
               "AlcoholYear",
               "DaysPhysHlthBad",
               "Poverty",
               "UrineFlow1",
               "DaysMentHlthBad"
)
Cont_complete = subset(data_complete, select = Cont_vars)
Cont_incomplete = subset(data_incomplete, select = Cont_vars)
Cont_incomplete_numeric <- as.data.frame(lapply(Cont_incomplete, as.numeric))
Cont_complete_numeric <- as.data.frame(lapply(Cont_complete, as.numeric))
```

Get mean and sd for complete cases

```{r}
Cont_complete_mean = apply(Cont_complete_numeric, 2, mean)
Cont_complete_sd = apply(Cont_complete_numeric, 2, sd)
```

Get mean and sd for incomplete cases

```{r}
Cont_incomplete_mean = apply(Cont_incomplete_numeric, 2, mean, na.rm = T)
Cont_incomplete_sd = apply(Cont_incomplete_numeric, 2, sd, na.rm = T)
```

Count # of complete cases for each continuous variable seperately in all complete cases dataset and incomplete cases dataset

```{r}
Cont_Number_complete = apply(Cont_complete_numeric, 2, function(x) sum(complete.cases(x)))
Cont_Number_incomplete = apply(Cont_incomplete_numeric, 2, function(x) sum(complete.cases(x)))
```

Determine whether the variance from complete cases and variance from incomplete cases are equal or not

```{r}
Cont_F_P_value = sapply(1:length(Cont_vars), function(x) var.test(Cont_complete_numeric[,x], Cont_incomplete_numeric[,x], alternative = "two.sided")$p.value>=0.05)
```

Get p-values from two-sample t-tests

```{r}
Cont_P_value = sapply(1:length(Cont_vars), function(x) t.test(Cont_complete_numeric[,x], Cont_incomplete_numeric[,x], var.equal = Cont_F_P_value[x])$p.value)
```

Generate the summary table used to fill in Table1

```{r}
Cont_summary = data.frame(cbind(Cont_Number_complete, Cont_complete_mean, Cont_complete_sd,
                                Cont_Number_incomplete, Cont_incomplete_mean, Cont_incomplete_sd,
                                Cont_P_value))
format(Cont_summary, nsmall = 2) ## only keep the first two decimals
```

Categorical/binary variables: get proportion and p-values from χ2 tests

Binary variables

Prepare data

```{r}
Bi_vars = c("Gender", "Smoke100",  "PhysActive")
Bi_complete = subset(data_complete, select = Bi_vars)
Bi_incomplete = subset(data_incomplete, select = Bi_vars)
Bi_data = subset(data2, select = c(Bi_vars, "Incomplete"))
```

Count # of complete cases for each binary variable seperately in all complete cases dataset and incomplete cases dataset

```{r}
Bi_Number_complete = apply(Bi_complete, 2, function(x) sum(complete.cases(x)))
Bi_Number_incomplete = apply(Bi_incomplete, 2, function(x) sum(complete.cases(x)))
```

Calculate the proportions of cases with level=1 for each binary variable seperately in all complete cases dataset and incomplete cases dataset

```{r}
Bi_prop_complete = sapply(1:length(Bi_vars), function(x) prop.table(table(Bi_complete[,x]))[2])
Bi_prop_incomplete = sapply(1:length(Bi_vars), function(x) prop.table(table(Bi_incomplete[,x]))[2])
```

Get p-values from χ2 tests

```{r}
Bi_P_value = sapply(Bi_vars, function(x) chisq.test(table(Bi_data[[x]], Bi_data$Incomplete), correct = FALSE)$p.value)
```

Generate the summary table used to fill in Table1

```{r}
Bi_summary = data.frame(cbind(Bi_Number_complete, Bi_prop_complete, Bi_Number_incomplete,
                              Bi_prop_incomplete, Bi_P_value))
format(Bi_summary, nsmall = 2) ## only keep the first two decimals
```

Categorical variables

Prepare data

```{r}
Cat_vars = c("HealthGen", "Race1")
Cat_complete = subset(data_complete, select = Cat_vars)
Cat_incomplete = subset(data_incomplete, select = Cat_vars)
Cat_data = subset(data2, select = c(Cat_vars, "Incomplete"))
```

Count # of complete cases for each categorical variable seperately in all complete cases dataset and incomplete cases dataset

```{r}
Cat_Number_complete = apply(Cat_complete, 2, function(x) sum(complete.cases(x)))
Cat_Number_incomplete = apply(Cat_incomplete, 2, function(x) sum(complete.cases(x)))
```

Calculate the proportions of each level for each categorical variable seperately in all complete cases dataset and incomplete cases dataset

```{r}
Cat_prop_complete = t(apply(Cat_complete, 2, function(x) prop.table(table(x))))
Cat_prop_incomplete = t(apply(Cat_incomplete, 2, function(x) prop.table(table(x))))
```

Get p-values from χ2 tests

```{r}
Cat_P_value = sapply(1:length(Cat_vars), function(x) chisq.test(table(Cat_data[[Cat_vars[x]]], Cat_data$Incomplete), correct = F)$p.value)
```

Generate the summary table used to fill in Table1

```{r}
Cat_summary = data.frame(cbind(Cat_Number_complete, Cat_prop_complete,
                               Cat_Number_incomplete,
                               Cat_prop_incomplete, Cat_P_value), check.names = F)
format(Cat_summary, nsmall = 2) ## only keep the first two decimals
```

Basic characteritics ########################################

```{r warning=FALSE}
rm(list = ls())
gc()
set.seed(123)
library(car)
library(olsrr)
library(ggplot2)
library(lmtest)
```

(1) Data cleaning ########################################

select variables

```{r warning=FALSE}
library(NHANES)
df0 <- NHANES
df <- NHANES[NHANES$Age >= 18 & NHANES$Age < 60,]
```

colSums(is.na(df)) / nrow(df)

```{r}
df <- df[, which(colSums(is.na(df)) / nrow(df) < 0.3)]
```

exclude duplication

```{r}
df <- df[!duplicated(df),]
names(df)
```

df\$BPSysAve

```{r warning=FALSE}
library(dplyr)
df2 <- df %>% select(
  SleepHrsNight,
  BMI,
  Age,
  Gender,
  Race1,
  TotChol,
  BPDiaAve,
  BPSysAve,
  AlcoholYear,
  Poverty,
  DaysMentHlthBad,
  UrineFlow1,
  PhysActive,
  DaysPhysHlthBad,
  Smoke100,
  HealthGen
)
df3 <- na.omit(df2)
library(dplyr)
```

(2) Data categorize ########################################

```{r warning=FALSE}
library(gtsummary)
library(kableExtra)
df3 <- df3 %>%
  mutate(BMIcat = case_when(
    BMI < 18  ~ "<18",
    BMI >= 18 & BMI <= 30 ~ "18-30",
    BMI > 30  ~ ">30"
  ),
  BMIcat = factor(BMIcat, levels = c("<18", "18-30", ">30")))
```

(3) generate table ########################################

```{r}
table_basic <- df3 %>%
  tbl_summary(
    by = BMIcat,
    statistic = list(all_continuous()  ~ "{mean} ({sd})",
                     all_categorical() ~ "{n}    ({p}%)"),
    digits = list(all_continuous()  ~ c(2, 2),
                  all_categorical() ~ c(0, 1)),
    missing = "no"
  ) %>%
  modify_header(
    label = "**Variable**",
    all_stat_cols() ~ "**{level}**<br>N = {n} ({style_percent(p, digits=1)}%)"
  ) %>%
  modify_caption("Participant characteristics, by BMI category") %>%
  bold_labels() %>%
  add_overall(col_label = "**All participants**<br>N = {N}") %>%
  add_p(
    test = list(
      all_continuous() ~ "kruskal.test",
      all_categorical() ~ "chisq.test"
    ),
    pvalue_fun = function(x) style_pvalue(x, digits = 3)
  ) %>%
  modify_footnote(
    all_stat_cols() ~ "p-values from Kruskal-Wallis rank sum test"
  )
```

View the table

```{r eval=FALSE}
# print(table_basic)
```

(4) convert to latex ########################################

Assuming 'table1' is already created using gtsummary

Convert to a kableExtra object

```{r eval=FALSE}
kable_table <- as_kable(table_basic)
```

Assuming 'kable_table' is already created using gtsummary

Convert to a kableExtra object

```{r eval=FALSE}
latex_table <- kable_table %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

Save the LaTeX table to file

```{r eval=FALSE}
save_kable(file = "table_for_overleaf.tex", latex_table)
```

MLR & Diagnosis no log ########################################

```{r warning=FALSE}
rm(list = ls())
gc()
set.seed(123)
library(car)
library(olsrr)
library(ggplot2)
library(lmtest)
```

(1) Data cleaning ########################################

select variables

```{r warning=FALSE}
library(NHANES)
df0 <- NHANES
df <- NHANES[NHANES$Age >= 18 & NHANES$Age < 60,]
```

colSums(is.na(df)) / nrow(df)

```{r}
df <- df[, which(colSums(is.na(df)) / nrow(df) < 0.3)]
```

exclude duplication

```{r}
df <- df[!duplicated(df),]
names(df)
```

df$BPSysAve

```{r warning=FALSE}
library(dplyr)
df2 <- df %>% select(
  SleepHrsNight,
  BMI,
  Age,
  Gender,
  Race1,
  TotChol,
  BPDiaAve,
  BPSysAve,
  AlcoholYear,
  Poverty,
  DaysMentHlthBad,
  UrineFlow1,
  PhysActive,
  DaysPhysHlthBad,
  Smoke100,
  HealthGen
)
df3 <- na.omit(df2)
```

df3\$SleepHrsNight <- df3\$SleepHrsNight * 60

df3 <- df3[, -which(names(df3) %in% "SleepHrsNight")]

cor(df3\$BPSysAve,df3\$BPDiaAve)

```{r}
psych::describe(df3)
```

psych::pairs.panels(df3)

```{r}
hist(df3$SleepHrsNight)
```

colSums(is.na(df2)) / nrow(df2)

```{r}
fit0 <-
  lm(SleepHrsNight ~ .,
     data = df3)
```

data type

```{r}
df3$Gender <- ifelse(df3$Gender == "male", 0, 1)
df3$Smoke100 <- ifelse(df3$Smoke100 == "No", 0, 1)
df3$PhysActive <- ifelse(df3$PhysActive == "No", 0, 1)
df3 <- df3 %>%
  mutate(
    Race1 = case_when(
      Race1 == 'Black' ~ 1,
      Race1 == 'Hispanic' ~ 2,
      Race1 == 'Mexican' ~ 3,
      Race1 == 'White' ~ 4,
      Race1 == 'Other' ~ 5,
      TRUE ~ NA_integer_  # Default value if none of the conditions are met
    )
  )
df3 <- df3 %>%
  mutate(
    HealthGen = case_when(
      HealthGen == 'Poor' ~ 1,
      HealthGen == 'Fair' ~ 2,
      HealthGen == 'Good' ~ 3,
      HealthGen == 'Vgood' ~ 4,
      HealthGen == 'Excellent' ~ 5,
      TRUE ~ NA_integer_  # Default value if none of the conditions are met
    )
  )
```

model_3 add additional risk factors ##

```{r}
m_3 = lm(
  BMI ~ SleepHrsNight + Age + Gender + factor(Race1)  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 + DaysMentHlthBad +
    DaysPhysHlthBad + factor(HealthGen) + PhysActive,
  df3
)
summary(m_3)
car::Anova(m_3, type = "III")
```

model 3 diagnosis ###########

```{r}
par(mfrow = c(2, 3)) #read more from ?plot.lm
plot(m_3, which = 1)
plot(m_3, which = 2)
plot(m_3, which = 3)
plot(m_3, which = 4)
plot(m_3, which = 5)
plot(m_3, which = 6)
par(mfrow = c(1, 1)) # reset
m_3.yhat = m_3$fitted.values
m_3.res = m_3$residuals
m_3.h = hatvalues(m_3)
m_3.r = rstandard(m_3)
m_3.rr = rstudent(m_3)
```

which subject is most outlying with respect to the x space

```{r}
Hmisc::describe(m_3.h)
m_3.h[which.max(m_3.h)]
```

Assumption:LINE ##############################

(1)Linear: 2 approaches

partial regression plots

```{r}
car::avPlots(m_3)
```

(2)Independence:

```{r}
residuals <- resid(m_3)
acf(residuals, main = "Autocorrelation Function of Residuals")
pacf(residuals, main = "Partial Autocorrelation Function of Residuals")
dw_test <- dwtest(m_3)
print(dw_test)
```

(3)E: constant var: residuals-fitted values; transform for variance-stable...(total: 4 solutions)

```{r}
car::residualPlots(m_3, type = "response")
plot(m_3, which = 1)
```

or

```{r}
ggplot(m_3, aes(x = m_3.yhat, y = m_3.res)) +
  geom_point(color = "blue", alpha = 0.8) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "black") +
  labs(title = "constant variance assumption",
       x = "y hat",
       y = "Residuals") +
  theme_minimal()
```

conclusion: the constant variance assumption is basically not violated. The spread of the residuals appears to be fairly uniform across the range of predicted values, the assumption is more likely to hold

(4)Normality: residuals freq - residuals (4 plots: his, box, Q-Q, shapiro); transform

exam quartiles of the residuals

```{r}
Hmisc::describe(m_3.res)
Hmisc::describe(m_3.res)$counts[c(".25", ".50", ".75")] #not symmetric
```

histogram

```{r}
par(mfrow = c(1, 1))
hist(m_3.res, breaks = 15)
```

Q-Q plot

```{r}
qq.m_3.res = car::qqPlot(m_3.res)
m_3.res[qq.m_3.res]
```

influential observations  #################

```{r}
influence2 = data.frame(
  Residual = resid(m_3),
  Rstudent = rstudent(m_3),
  HatDiagH = hat(model.matrix(m_3)),
  CovRatio = covratio(m_3),
  DFFITS = dffits(m_3),
  COOKsDistance = cooks.distance(m_3)
)
```

DFFITS

```{r}
ols_plot_dffits(m_3)
influence2[order(abs(influence2$DFFITS), decreasing = T), ] %>% head()
```

From the plot above, we can see 2 observations with the largest (magnitude) of DFFITS, observation 879 and 1769 By printing the corresponding values of DFFITS in the output dataset, we can obtain their DFFITS values: 0.5673 for observation 879, 0.5872 for observation 1769

Cook's D

```{r}
ols_plot_cooksd_bar(m_3)
influence2[order(influence2$COOKsDistance, decreasing = T), ] %>% head()
```

From the plot above, we can see that the observation 879 and 1769 also have the largest Cook’s Distance. By printing the corresponding values of Cook’s D in the output dataset, we can obtain their Cook’s D values:0.0108 for observation 879, 0.0145 for observation 1769

leverage

```{r}
ols_plot_resid_lev(m_3)
```

high leverage

```{r}
influence2[order(influence2$HatDiagH, decreasing = T), ] %>% head()
```

high studentized residual

```{r}
influence2[order(influence2$Rstudent, decreasing = T), ] %>% head()
```

From the plot above, we can see that the observation 1155 has the largest leverage (0.0368). Observations 1862 has the largest (in magnitude) externally studentized residual (5.9649).

From the plot above, there is 7 observations(1048,1769,1684, 74, 72, 1689, 1311) located in the intersection areas of both outlier and leverage, which is to say, those observations has both the leverage and the externally studentized residual exceeding their respective thresholds.Due to its large DIFFITS and Cook’s D, they are potentially influential observations.

The thresholds for the externally studentized residual are -2 and 2, i.e. 2 in magnitude. The thresholds for the leverage of the R default is 0.011

From (DFFITS), observations 879 and 1769 appear to be influential observations. Observation 1155 has extraordinarily large leverage. Therefore, I choose to remove these 14 observations in the re-fitted mode

```{r}
rm3.df3 = df3[-c(170, 208, 444, 926, 1361, 1454, 1546, 1655, 1910, 1958), ]
rm.m_3 =  lm(
  BMI ~ SleepHrsNight + Age + Gender + Race1  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 + DaysMentHlthBad +
    DaysPhysHlthBad + HealthGen + PhysActive,
  rm3.df3
)
```

Before removing these observations, the estimated coefficients are:

```{r}
summary(m_3)$coef
```

After removing these observations, the estimated coefficients are:

```{r}
summary(rm.m_3)$coef
```

change percent

```{r}
abs((rm.m_3$coefficients - m_3$coefficients) / (m_3$coefficients) * 100)
```

The estimated regression coefficients doesn't change slightly after removing these observations. 5 of the estimates have changed by more than 10% after calculation. The p-value for the coefficient forSleepHrsNight    is becoming insignificant with 95% confidence level.

multicollinearity   ######################

Pearson correlations

```{r}
var3 = c(
  "BMI",
  "SleepHrsNight",
  "Age",
  "Gender",
  "Race1",
  "TotChol",
  "BPDiaAve",
  "BPSysAve",
  "AlcoholYear",
  "Smoke100",
  "DaysPhysHlthBad",
  "PhysActive",
  "Poverty",
  "UrineFlow1",
  "DaysMentHlthBad",
  "HealthGen"
)
newData3 = df3[, var3]
library("corrplot")
par(mfrow = c(1, 2))
cormat3 = cor(as.matrix(newData3[, -c(1)], method = "pearson"))
p.mat3 = cor.mtest(as.matrix(newData3[, -c(1)]))$p
corrplot(
  cormat3,
  method = "color",
  type = "upper",
  number.cex = 1,
  diag = FALSE,
  addCoef.col = "black",
  tl.col = "black",
  tl.srt = 90,
  p.mat = p.mat3,
  sig.level = 0.05,
  insig = "blank",
)
```

None of the covariates seem strongly correlated.There is no evidence of collinearity from the pair-wise correlations.

collinearity diagnostics (VIF)

```{r}
car::vif(m_3)
```

From the VIF values in the output above, once again we do not observe any potential collinearity issues. In fact, the VIF values are fairly small: none of the values exceed 10.

MLR & Diagnosis log ########################################

model_3 add additional risk factors ##

```{r}
df3$logBMI = log(df3$BMI + 1)
m_3 = lm(
  logBMI ~ SleepHrsNight + DaysMentHlthBad+Age + Gender + factor(Race1)  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 +
    DaysPhysHlthBad + factor(HealthGen) + PhysActive,
  df3
)
summary(m_3)
car::Anova(m_3, type = "III")
```

model 3 diagnosis ###########

```{r}
par(mfrow = c(2, 3)) #read more from ?plot.lm
plot(m_3, which = 1)
plot(m_3, which = 2)
plot(m_3, which = 3)
plot(m_3, which = 4)
plot(m_3, which = 5)
plot(m_3, which = 6)
par(mfrow = c(1, 1)) # reset
m_3.yhat = m_3$fitted.values
m_3.res = m_3$residuals
m_3.h = hatvalues(m_3)
m_3.r = rstandard(m_3)
m_3.rr = rstudent(m_3)
```

which subject is most outlying with respect to the x space

```{r}
Hmisc::describe(m_3.h)
m_3.h[which.max(m_3.h)]
```

Assumption:LINE ##############################

(1)Linear: 2 approaches

partial regression plots

```{r}
car::avPlots(m_3)
```

(2)Independence:

```{r}
residuals <- resid(m_3)
acf(residuals, main = "Autocorrelation Function of Residuals")
pacf(residuals, main = "Partial Autocorrelation Function of Residuals")
dw_test <- dwtest(m_3)
print(dw_test)
```

(3)E: constant var: residuals-fitted values; transform for variance-stable...(total: 4 solutions)

```{r}
car::residualPlots(m_3, type = "response")
plot(m_3, which = 1)
```

or

```{r}
ggplot(m_3, aes(x = m_3.yhat, y = m_3.res)) +
  geom_point(color = "blue", alpha = 0.8) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "black") +
  labs(title = "constant variance assumption",
       x = "y hat",
       y = "Residuals") +
  theme_minimal()
```

conclusion: the constant variance assumption is basically not violated. The spread of the residuals appears to be fairly uniform across the range of predicted values, the assumption is more likely to hold

(4)Normality: residuals freq - residuals (4 plots: his, box, Q-Q, shapiro); transform

exam quartiles of the residuals

```{r}
Hmisc::describe(m_3.res)
Hmisc::describe(m_3.res)$counts[c(".25", ".50", ".75")] #not symmetric
```

histogram

```{r}
par(mfrow = c(1, 1))
hist(m_3.res, breaks = 15)
```

Q-Q plot

```{r}
qq.m_3.res = car::qqPlot(m_3.res)
m_3.res[qq.m_3.res]
```

influential observations  #################

```{r}
influence3 = data.frame(
  Residual = resid(m_3),
  Rstudent = rstudent(m_3),
  HatDiagH = hat(model.matrix(m_3)),
  CovRatio = covratio(m_3),
  DFFITS = dffits(m_3),
  COOKsDistance = cooks.distance(m_3)
)
```

DFFITS

```{r}
ols_plot_dffits(m_3)
influence3[order(abs(influence3$DFFITS), decreasing = T), ] %>% head()
```

From the plot above, we can see 2 observations with the largest (magnitude) of DFFITS, observation 879 and 1769 By printing the corresponding values of DFFITS in the output dataset, we can obtain their DFFITS values: 0.5673 for observation 879, 0.5872 for observation 1769

Cook's D

```{r}
ols_plot_cooksd_bar(m_3)
influence3[order(influence3$COOKsDistance, decreasing = T), ] %>% head()
```

From the plot above, we can see that the observation 879 and 1769 also have the largest Cook’s Distance. By printing the corresponding values of Cook’s D in the output dataset, we can obtain their Cook’s D values:0.0108 for observation 879, 0.0145 for observation 1769

leverage

```{r}
ols_plot_resid_lev(m_3)
```

high leverage

```{r}
influence3[order(influence3$HatDiagH, decreasing = T), ] %>% head()
```

high studentized residual

```{r}
influence3[order(influence3$Rstudent, decreasing = T), ] %>% head()
```

From the plot above, we can see that the observation 1155 has the largest leverage (0.0368). Observations 1862 has the largest (in magnitude) externally studentized residual (5.9649).

From the plot above, there is 7 observations(1048,1769,1684, 74, 72, 1689, 1311) located in the intersection areas of both outlier and leverage, which is to say, those observations has both the leverage and the externally studentized residual exceeding their respective thresholds.Due to its large DIFFITS and Cook’s D, they are potentially influential observations.

The thresholds for the externally studentized residual are -2 and 2, i.e. 2 in magnitude. The thresholds for the leverage of the R default is 0.011

From (DFFITS), observations 879 and 1769 appear to be influential observations. Observation 1155 has extraordinarily large leverage. Therefore, I choose to remove these 14 observations in the re-fitted mode

```{r}
rm3.df3 = df3[-c(444, 926, 1348, 1454, 1655, 1910, 1958), ]
rm.m_3 =  lm(
  logBMI ~ SleepHrsNight + Age + Gender + Race1  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 + DaysMentHlthBad +
    DaysPhysHlthBad + HealthGen + PhysActive,
  rm3.df3
)
```

Before removing these observations, the estimated coefficients are:

```{r}
summary(m_3)$coef
```

After removing these observations, the estimated coefficients are:

```{r}
summary(rm.m_3)$coef
```

change percent

```{r}
abs((rm.m_3$coefficients - m_3$coefficients) / (m_3$coefficients) * 100)
```

The estimated regression coefficients doesn't change slightly after removing these observations. 5 of the estimates have changed by more than 10% after calculation. The p-value for the coefficient forSleepHrsNight    is becoming insignificant with 95% confidence level.

multicollinearity   ######################

Pearson correlations

```{r}
var3 = c(
  "BMI",
  "SleepHrsNight",
  "Age",
  "Gender",
  "Race1",
  "TotChol",
  "BPDiaAve",
  "BPSysAve",
  "AlcoholYear",
  "Smoke100",
  "DaysPhysHlthBad",
  "PhysActive",
  "Poverty",
  "UrineFlow1",
  "DaysMentHlthBad",
  "HealthGen"
)
newData3 = df3[, var3]
library("corrplot")
par(mfrow = c(1, 2))
cormat3 = cor(as.matrix(newData3[, -c(1)], method = "pearson"))
p.mat3 = cor.mtest(as.matrix(newData3[, -c(1)]))$p
corrplot(
  cormat3,
  method = "color",
  type = "upper",
  number.cex = 1,
  diag = FALSE,
  addCoef.col = "black",
  tl.col = "black",
  tl.srt = 90,
  p.mat = p.mat3,
  sig.level = 0.05,
  insig = "blank",
)
```

None of the covariates seem strongly correlated.There is no evidence of collinearity from the pair-wise correlations.

collinearity diagnostics (VIF)

```{r}
car::vif(m_3)
```

From the VIF values in the output above, once again we do not observe any potential collinearity issues. In fact, the VIF values are fairly small: none of the values exceed 10.

MLR & Diagnosis with interaction ########################################

model_4 add additional risk factors ##

```{r}
df3$logBMI = log(df3$BMI + 1)
m_full = lm(
  logBMI ~ SleepHrsNight + DaysMentHlthBad+ Age + Gender + factor(Race1)  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 +
    DaysPhysHlthBad + factor(HealthGen) + PhysActive + SleepHrsNight*Age + SleepHrsNight*Gender,
  df3
)
summary(m_full)
car::Anova(m_full, type = "III")
Confint(m_full)
```

model 4 diagnosis ###########

```{r}
par(mfrow = c(2, 3)) #read more from ?plot.lm
plot(m_full, which = 1)
plot(m_full, which = 2)
plot(m_full, which = 3)
plot(m_full, which = 4)
plot(m_full, which = 5)
plot(m_full, which = 6)
par(mfrow = c(1, 1)) # reset
m_full.yhat = m_full$fitted.values
m_full.res = m_full$residuals
m_full.h = hatvalues(m_full)
m_full.r = rstandard(m_full)
m_full.rr = rstudent(m_full)
```

which subject is most outlying with respect to the x space

```{r}
Hmisc::describe(m_full.h)
m_full.h[which.max(m_full.h)]
```

Assumption:LINE ##############################

(1)Linear: 2 approaches

partial regression plots

```{r}
car::avPlots(m_full)
```

(2)Independence:

```{r}
residuals <- resid(m_full)
acf(residuals, main = "Autocorrelation Function of Residuals")
pacf(residuals, main = "Partial Autocorrelation Function of Residuals")
dw_test <- dwtest(m_full)
print(dw_test)
```

(3)E: constant var: residuals-fitted values; transform for variance-stable...(total: 4 solutions)

```{r}
car::residualPlots(m_full, type = "response")
plot(m_full, which = 1)
```

or

```{r}
ggplot(m_full, aes(x = m_full.yhat, y = m_full.res)) +
  geom_point(color = "blue", alpha = 0.8) +
  geom_hline(yintercept = 0,
             linetype = "dashed",
             color = "black") +
  labs(title = "constant variance assumption",
       x = "y hat",
       y = "Residuals") +
  theme_minimal()
```

conclusion: the constant variance assumption is basically not violated. The spread of the residuals appears to be fairly uniform across the range of predicted values, the assumption is more likely to hold

(4)Normality: residuals freq - residuals (4 plots: his, box, Q-Q, shapiro); transform

exam quartiles of the residuals

```{r}
Hmisc::describe(m_full.res)
Hmisc::describe(m_full.res)$counts[c(".25", ".50", ".75")] #not symmetric
```

histogram

```{r}
par(mfrow = c(1, 1))
hist(m_full.res, breaks = 15)
```

Q-Q plot

```{r}
qq.m_full.res = car::qqPlot(m_full.res)
m_full.res[qq.m_full.res]
```

influential observations  #################

```{r}
influence4 = data.frame(
  Residual = resid(m_full),
  Rstudent = rstudent(m_full),
  HatDiagH = hat(model.matrix(m_full)),
  CovRatio = covratio(m_full),
  DFFITS = dffits(m_full),
  COOKsDistance = cooks.distance(m_full)
)
```

DFFITS

```{r}
ols_plot_dffits(m_full)
influence4[order(abs(influence4$DFFITS), decreasing = T), ] %>% head()
```

From the plot above, we can see 2 observations with the largest (magnitude) of DFFITS, observation 879 and 1769 By printing the corresponding values of DFFITS in the output dataset, we can obtain their DFFITS values: 0.5673 for observation 879, 0.5872 for observation 1769

Cook's D

```{r}
ols_plot_cooksd_bar(m_full)
influence4[order(influence4$COOKsDistance,decreasing = T),] %>% head()
```

From the plot above, there is 7 observations(1048,1769,1684, 74, 72, 1689, 1311) located in the intersection areas of both outlier and leverage, which is to say, those observations has both the leverage and the externally studentized residual exceeding their respective thresholds.Due to its large DIFFITS and Cook's D, they are potentially influential observations.

The thresholds for the externally studentized residual are -2 and 2, i.e. 2 in magnitude. The thresholds for the leverage of the R default is 0.011

leverage

```{r}
ols_plot_resid_lev(m_full)
```

high leverage

```{r}
influence4[order(influence4$HatDiagH,decreasing = T),] %>% head()
```

high studentized residual

```{r}
influence4[order(influence4$Rstudent,decreasing = T),] %>% head()
```

From (DFFITS), observations 879 and 1769 appear to be influential observations. Observation 1155 has extraordinarily large leverage. Therefore, I choose to remove these 14 observations in the re-fitted mode

```{r}
rm4.df3 = df3[-c(1048, 1769, 1684, 74, 72, 1689, 1311, 879), ]
rm.m_full =  lm(
  logBMI ~ SleepHrsNight + Age + Gender + factor(Race1)  + Poverty + TotChol + BPDiaAve + BPSysAve + AlcoholYear + Smoke100 + UrineFlow1 + DaysMentHlthBad +
    DaysPhysHlthBad + factor(HealthGen) + PhysActive + SleepHrsNight*Age + SleepHrsNight*Gender,
  rm4.df3
)
```

Before removing these observations, the estimated coefficients are:

```{r}
summary(m_full)$coef
```

After removing these observations, the estimated coefficients are:

```{r}
summary(rm.m_full)$coef
```

change percent

```{r}
abs((rm.m_full$coefficients - m_full$coefficients) / (m_full$coefficients) * 100)
```

The estimated regression coefficients doesn't change slightly after removing these observations. 5 of the estimates have changed by more than 10% after calculation. The p-value for the coefficient forSleepHrsNight    is becoming insignificant with 95% confidence level.

multicollinearity   ######################

Pearson correlations

```{r}
var4 = c(
  "BMI",
  "SleepHrsNight",
  "Age",
  "Gender",
  "Race1",
  "TotChol",
  "BPDiaAve",
  "BPSysAve",
  "AlcoholYear",
  "Smoke100",
  "DaysPhysHlthBad",
  "PhysActive",
  "Poverty",
  "UrineFlow1",
  "DaysMentHlthBad",
  "HealthGen"
)
newData4 = df3[, var4]
library("corrplot")
par(mfrow = c(1, 2))
cormat4 = cor(as.matrix(newData4[, -c(1)], method = "pearson"))
p.mat4 = cor.mtest(as.matrix(newData4[, -c(1)]))$p
corrplot(
  cormat4,
  method = "color",
  type = "upper",
  number.cex = 1,
  diag = FALSE,
  addCoef.col = "black",
  tl.col = "black",
  tl.srt = 90,
  p.mat = p.mat4,
  sig.level = 0.05,
  insig = "blank",
)
```

None of the covariates seem strongly correlated.There is no evidence of collinearity from the pair-wise correlations.

collinearity diagnostics (VIF)

```{r}
car::vif(m_full)
```

From the VIF values in the output above, once again we do not observe any potential collinearity issues. In fact, the VIF values are fairly small: none of the values exceed 10.

```{r}
getMode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
new_data <- expand.grid(SleepHrsNight = seq(min(df3$SleepHrsNight), max(df3$SleepHrsNight), length.out = 100),
                        Age = quantile(df3$Age, probs = c(0.25, 0.5, 0.75)),
                        Gender = median(df3$Gender, na.rm = TRUE),
                        Race1 = median(df3$Race1, na.rm = TRUE),
                        Poverty = median(df3$Poverty, na.rm = TRUE),
                        TotChol = median(df3$TotChol, na.rm = TRUE),
                        BPDiaAve = median(df3$BPDiaAve, na.rm = TRUE),
                        BPSysAve = median(df3$BPSysAve, na.rm = TRUE),
                        AlcoholYear = median(df3$AlcoholYear, na.rm = TRUE),
                        Smoke100 = getMode(df3$Smoke100),
                        UrineFlow1 = median(df3$UrineFlow1, na.rm = TRUE),
                        DaysMentHlthBad = median(df3$DaysMentHlthBad, na.rm = TRUE),
                        DaysPhysHlthBad = median(df3$DaysPhysHlthBad, na.rm = TRUE),
                        HealthGen = getMode(df3$HealthGen),
                        PhysActive = getMode(df3$PhysActive)
)
```

predict

```{r}
new_data$predicted_BMI <- predict(m_full, newdata = new_data)
```

interaction

```{r}
library(ggplot2)
ggplot(new_data, aes(x = SleepHrsNight, y = predicted_BMI, group = factor(Age))) +
  geom_line(aes(color = factor(Age))) +
  labs(title = "Interaction between Sleep Hours and Age on BMI",
       x = "Sleep Hours per Night",
       y = "Predicted BMI")
library(ggplot2)
ggplot(new_data, aes(x = SleepHrsNight, y = predicted_BMI, group = factor(Gender))) +
  geom_line(aes(color = factor(Gender))) +
  labs(title = "Interaction between Sleep Hours and Gender on BMI",
       x = "Sleep Hours per Night",
       y = "Predicted BMI")
```

cross validation

```{r}
library(caret)
splitIndex <-
  createDataPartition(df3$SleepHrsNight, p = 0.7, list = FALSE)
trainData <- df3[splitIndex, ]
testData <- df3[-splitIndex, ]
predictions <- predict(m_full, newdata = testData)
mse <- mean((testData$SleepHrsNight - predictions) ^ 2)
control <-
  trainControl(method = "cv", number = 10)  # 10-fold cross-validation
cv_model <-
  train(
    SleepHrsNight ~ .,
    data = df3,
    method = "lm",
    trControl = control
  )
cv_model
(cv_results <- cv_model$results)
```

